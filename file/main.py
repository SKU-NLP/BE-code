import pandas as pd
import numpy as np
import ast
import re
import os

from sentence_transformers import SentenceTransformer
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from openai import OpenAI
from dotenv import load_dotenv

# ======================================================
# 0. ê¸°ë³¸ ì„¤ì •
# ======================================================
load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

app = FastAPI(title="ë§¥ë½ ê¸°ë°˜ ëŒ€í•™ ì¶”ì²œ API")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# ======================================================
# 1. í…ìŠ¤íŠ¸ ì •ê·œí™”
# ======================================================
def normalize_text(text: str) -> str:
    text = str(text)
    text = re.sub(r"[,\u00b7ãƒ»]", " ", text)
    text = re.sub(r"\s+", " ", text)
    return text.strip()

# ======================================================
# 2. í…ìŠ¤íŠ¸ + ì„ë² ë”© DB ë¡œë”© (í•µì‹¬ ìˆ˜ì • ë¶€ë¶„)
# ======================================================
print("ğŸ“‚ í…ìŠ¤íŠ¸ + ì„ë² ë”© DB ë¡œë”© ì¤‘...")

# â‘  ì›ë³¸ í…ìŠ¤íŠ¸ ë°ì´í„°
text_df = pd.read_csv("language.csv").fillna("")
text_df.columns = text_df.columns.str.strip()

# â‘¡ ì„ë² ë”© ë°ì´í„°
emb_df = pd.read_csv("test_language.csv").fillna("")
emb_df["embedding"] = emb_df["embedding"].apply(
    lambda x: np.array(ast.literal_eval(x))
)

# â‘¢ í–‰ ìˆ˜ ê²€ì¦
if len(text_df) != len(emb_df):
    raise ValueError("âŒ language.csv ì™€ test_language.csv í–‰ ê°œìˆ˜ê°€ ë‹¤ë¦…ë‹ˆë‹¤.")

# â‘£ ê²°í•© (í–‰ ìˆœì„œ ê¸°ì¤€)
df = text_df.copy()
df["embedding"] = emb_df["embedding"].values

# â‘¤ ì§€ì—­ ì»¬ëŸ¼ í†µí•©
if "ì†Œì¬ì§€(ìƒì„¸)" in df.columns:
    df["ì§€ì—­"] = df["ì†Œì¬ì§€(ìƒì„¸)"]
elif "ì†Œì¬ì§€" in df.columns:
    df["ì§€ì—­"] = df["ì†Œì¬ì§€"]
else:
    df["ì§€ì—­"] = ""

# â‘¥ ì„ë² ë”© í–‰ë ¬
corpus_embeddings = np.vstack(df["embedding"].values)

# â‘¦ ëª¨ë¸ ë¡œë”©
model = SentenceTransformer("intfloat/multilingual-e5-base")

print("âœ… DB ë¡œë”© ì™„ë£Œ")

# ======================================================
# 3. ë§¥ë½ ê¸°ë°˜ ê²€ìƒ‰ (ì„ë² ë”© ìœ ì‚¬ë„)
# ======================================================
def search_major_contextual(user_query: str, top_k: int = 5):
    query = "query: " + normalize_text(user_query)

    query_embedding = model.encode(
        query,
        convert_to_numpy=True,
        normalize_embeddings=True
    )

    df_work = df.copy()
    df_work["score"] = np.dot(corpus_embeddings, query_embedding)

    results = df_work.sort_values("score", ascending=False).head(top_k)

    # ì™„ì „ ë¬´ê´€ ì§ˆë¬¸ ë°©ì–´
    if results.iloc[0]["score"] < 0.65:
        return pd.DataFrame()

    return results

# ======================================================
# 4. GPT í•˜ì´ë¸Œë¦¬ë“œ í”„ë¡¬í”„íŠ¸
# ======================================================
def build_hybrid_prompt(user_query: str, results_df: pd.DataFrame):
    context = ""

    if not results_df.empty:
        for _, row in results_df.iterrows():
            context += (
                f"ëŒ€í•™: {row['ëŒ€í•™ëª…']} / "
                f"ë‹¨ê³¼ëŒ€í•™: {row['ë‹¨ê³¼ëŒ€í•™']} / "
                f"í•™ê³¼: {row['í•™ê³¼ëª…']} / "
                f"ì§€ì—­: {row['ì§€ì—­']} / "
                f"íŠ¹ì§•: {row['í•™ê³¼íŠ¹ì„±']} / "
                f"ê³„ì—´: {row['í‘œì¤€ë¶„ë¥˜ê³„ì—´(ì¤‘)']}\n"
            )

    return f"""
ë„ˆëŠ” í•œêµ­ì˜ ëŒ€í•™ ì…ì‹œ ë° ì§„ë¡œ ì „ë¬¸ ìƒë‹´ ì±—ë´‡ì´ë‹¤.

[ì‚¬ìš©ì ì§ˆë¬¸]
{user_query}

[ë‚´ë¶€ ì°¸ê³  ë°ì´í„°]
{context if context else "ì§ì ‘ì ì¸ í•™ê³¼ ë§¤ì¹­ ë°ì´í„°ëŠ” ì—†ì§€ë§Œ, ìœ ì‚¬ ê³„ì—´ì„ ê¸°ì¤€ìœ¼ë¡œ ì¶”ì²œí•´ë¼."}

[ë‹µë³€ ì§€ì¹¨]
1. ë‚´ë¶€ ë°ì´í„°ê°€ ìˆë‹¤ë©´ ë°˜ë“œì‹œ ê·¼ê±°ë¡œ í™œìš©í•´ë¼.
2. í•™ê³¼ ì„±ê²©, ì§„ë¡œ ë°©í–¥, ì·¨ì—… ë¶„ì•¼ë¥¼ í•¨ê»˜ ì„¤ëª…í•´ë¼.
3. ìˆ˜í—˜ìƒì˜ ìƒí™©ì„ ê³ ë ¤í•œ í˜„ì‹¤ì ì¸ ì¡°ì–¸ì„ í¬í•¨í•´ë¼.
4. ìì—°ìŠ¤ëŸ½ê³  ìƒë‹´í•˜ë“¯ ë‹µë³€í•´ë¼.
"""

# ======================================================
# 5. API ì—”ë“œí¬ì¸íŠ¸
# ======================================================
class ChatRequest(BaseModel):
    question: str

@app.post("/chat")
def chat(req: ChatRequest):
    results = search_major_contextual(req.question)
    prompt = build_hybrid_prompt(req.question, results)

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "ë„ˆëŠ” ì…ì‹œ ë°ì´í„°ì™€ ìƒì‹ì„ ê²°í•©í•´ ìƒë‹´í•˜ëŠ” ì „ë¬¸ê°€ë‹¤."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.5
    )

    return {
        "answer": response.choices[0].message.content,
        "matched_count": len(results)
    }
